{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction of Formation Energies and Bandgaps in Transparent Semiconductors<br>\n",
    "### Abhinav Malhotra, Mayank Agrawal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Machine learning theory has impacted a large variety of areas including computer gaming, fraud detection and bioinformatics<sup>1</sup>. Materials research is now a key thrust area for these data analysis techniques. Traditionally in material science, the role of computational approaches has been restricted in its predictive nature owing to the heavy computational requirements and associated costs (e.g. hundreds of CPU hours, expensive maintenance of hardware clusters) of tools such as molecular dynamics, density functional theory (DFT) calculations and ab-initio material simulations. More recently, promising work on prediction of phase diagrams,<sup>2</sup> crystal structures,<sup>3,4</sup> and materials properties,<sup>5,6</sup> and rapid data analysis of high‐throughput experiments,<sup>7</sup> has been documented in the literature. Further progress is expected to strengthen the role of machine learning accelerated materials design, research and prediction. Smarter search techniques using DFT generated datasets as the foundation for machine learning algorithms could hold potential to revolutionize the field and accelerate material discovery and design<sup>8</sup>. Thus, the use of data science tools can enable an efficient prediction of material structures with desired properties.<br><br>\n",
    "In this course project, we will apply machine learning tools to a DFT dataset of material structures with the aim of predicting properties of interest viz. formation energies and electronic bandgaps for transparent conducting materials. We will use both supervised and unsupervised learning paradigms of machine learning to derive correlations between structural information and material properties which could help accelerate the search and design of efficient transparent conductors. The motivation for this project stems from the fact that global climate challenge has brought a focus on generating energy from renewable and sustainable resources including the most abundant energy resource available to us in the form of the Sun. The conversion of solar energy to electric energy is hinged upon the performance of solar photovoltaic materials. Transparent conducting materials are an important class of materials used in the manufacture of these solar photovoltaics<sup>9</sup>. In addition, modern optoelectronic devices make use of this special class of materials. As the name suggests, transparent conductors are electrically conducting with low absorption of the visible spectra of light (see Figure 1(b) ). The absorptivity is controlled by the band gap between the conduction and the valence band. A larger electronic band-gap promotes the transparent property, but at the same time competes with the high electrical conductivity property<sup>10</sup>. The dataset under consideration comprises of oxides of Aluminum, Gallium and Indium, which are expected to lead the front in search for efficient transparent conductors owing to the combination of both large bandgap energies, which leads to optical transparency over the visible range, and low electrical resistance. These alloys are described by the formula $(Al_{x}Ga_{y}In_{z})_{2N}O_{3N}$; where x, y, and z can vary but are limited by the constraint x+y+z = 1. The total number of atoms in the unit cell, $N_{total}$=$2N$+$3N$ (where N is an integer), is typically between 5 and 100. However, the main limitation in the design of compounds is that identification and discovery of novel materials for targeted applications requires an examination of enormous compositional and configurational degrees of freedom (i.e., many combinations of x, y, and z). To avoid costly and inefficient trial-and-error of synthetic routes, computational data-driven methods can be used to guide the discovery of potentially more efficient materials to aid in the development of advanced (or totally new) technologies. In computational material science, the standard tool for computing these properties is the quantum-mechanical method known as density-functional theory (DFT) which is computationally expensive. As a result, potential $(Al_{x}Ga_{y}In_{z})_{2N}O_{3N}$ materials remain relatively unexplored. Data-driven models offer an alternative approach to efficiently search for new possible compounds in targeted applications but at a significantly reduced computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"band_gap.png\" width=\"900\">\n",
    "    </center>\n",
    "    <figcaption>Figure 1<sup>9</sup>. (a) A simplified Energy Bandgap in semiconductors is shown as energy difference Eg between conduction and valence band. (b) The Shockley–Queisser limit gives the maximum possible efficiency of a simple solar cell under sunlight, as a function of the semiconductor band gap. If the band gap is too high, most of the visible spectra cannot be absorbed; if it is too low, then photons have much more energy than necessary to excite electrons across the band gap, and the rest is wasted. </figcaption>\n",
    "\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description and Project Goals\n",
    "The Transparent Conductor Database contains raw data for materials with the information on their space group, total number of constituent atoms, lattice vectors and angles. Additionally, atomic structure files with the coordinates of all the atoms in each structure are provided in hierarchically arranged directories. Furthermore, formation energies and electronic bandgaps are provided for ~80% of the materials. Raw data was manipulated to csv files in the folder 'train' and 'test' respectively. All the data can be downloaded from the kaggle website link 'https://www.kaggle.com/c/nomad2018-predict-transparent-conductors/data'.\n",
    "\n",
    "### How does the working data look like?\n",
    "index: id = id of the crystal structure that ranges from 1 to 2400 for training dataset and 1 to 600 for test dataset.<br><br>\n",
    "\n",
    "<font color=\"red\">\n",
    "    0: spacegroup = spacegroup of the crystal structure\n",
    "</font>\n",
    "\n",
    "1: N_total = number of total atoms in the unit cell\n",
    "\n",
    "2: x_Al = percent_atom_al\n",
    "\n",
    "3: x_Ga = percent_atom_ga\n",
    "\n",
    "4: x_In = percent_atom_in\n",
    "\n",
    "5: a = lattice_vector_1_ang\n",
    "\n",
    "6: b = lattice_vector_2_ang\n",
    "\n",
    "7: c = lattice_vector_3_ang\n",
    "\n",
    "8: alpha = lattice_angle_alpha_degree\n",
    "\n",
    "9: beta = lattice_angle_beta_degree\n",
    "\n",
    "10: gamma = lattice_angle_gamma_degree <br> <br>\n",
    "\n",
    "<font color=\"blue\"> 11: del_Hf = formation_energy_ev_natom (available only for training data) <br><br>\n",
    "\n",
    "12: bandgap = bandgap_energy_ev (available only for training data) </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goals\n",
    "\n",
    "#### Goal 1: \n",
    "Feature selection for machine learning algorithms. We are using structural parameters and atomic positions as features. <br> Columns 1-10 serve as the foundation for features for machine learning algorithms in this project. \n",
    "\n",
    "#### Goal 2: \n",
    "Using the selected features (from Goal 1) we used classification algorithm to predict spacegroups. Note that del_Hf and bandgap are excluded from the dataset in this Goal. The purpose of this aspect of the project is to validate feature choices as well as implement classification algorithms learnt during the course.\n",
    "<br> Out of these 14 columns (including index), <font color=\"red\">column \"0:spacegroup\" is used as label in classiciation.</font>\n",
    "\n",
    "#### Goal 3: \n",
    "Using the features from Goal 1 and varied regression strategies, we are predicting formation energies and bandgaps for transparent conductors. Additionally, we will analyse the performance of these implemented learners.\n",
    "<br><font color=\"blue\">Columns \"11: del_Hf\" and \"12: bandgap\" are used as target values for regression.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-24T23:40:21.824234Z",
     "start_time": "2018-03-24T23:40:21.173895Z"
    },
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   #Need seaborn for heatmap\n",
    "\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "### Import training and test data into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "column_names=['spacegroup','N_total','x_Al','x_Ga','x_In','a','b','c','alpha', 'beta','gamma', 'del_Hf', 'bandgap']\n",
    "df_train = pd.read_csv('./train.csv', header=0, index_col = 0, names = column_names)\n",
    "df_test = pd.read_csv('./test.csv', header=0, index_col = 0, names = column_names[0:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset has the dimensions of 2400x13 with known values of del_Hf and bandgaps while the test dataset has the dimensions of 600x11 with unknown values of del_Hf and bandgaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overall dataset statistical decription "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gaining insights into data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is there correlation between bandgap and del_Hf? \n",
    "If a strong correlation exists between these two quanitites, then we can save on computation time by predicting one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bghf = sns.lmplot( x=\"del_Hf\", y = \"bandgap\", data = df_train, fit_reg = True, line_kws={'color': 'red'})\n",
    "print(\"The correlation b/w bandgap and formation energy is:{}\".format(df_train['bandgap'].corr(df_train['del_Hf'])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, these two quanitites are at best weakly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the distribution of the transparent conductor spacegroups in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacegroups = df_train.spacegroup.unique()\n",
    "spacegroups.sort()\n",
    "n_spacegroups = len(spacegroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train['spacegroup'].value_counts(normalize=False).plot.bar()\n",
    "plt.ylabel('# of crystals in spacegroup')\n",
    "plt.xlabel('Space group')\n",
    "plt.title(\"Distribution of crystals in dataset among spacegroups\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole dataset consists of 6 types of spacegroups, the number of structures corresponding to each space groups are shown in the graph above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets look at the distribution of spacegroups with final quanitites of interest: formation energy and bandgaps.\n",
    "\n",
    "We are using plotly for interactive plots. Each plot displays the statistical parameters within each group upon mouseover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for number_of_total_atoms in df_train['spacegroup'].value_counts().index.tolist():\n",
    "    y0 = df_train[df_train['spacegroup']==number_of_total_atoms]['del_Hf'].values\n",
    "    data.append(go.Box(y=y0, name=str(number_of_total_atoms), boxpoints = 'suspectedoutliers',boxmean='sd'))\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        title = \"Spacegroup vs. Formation energy\",\n",
    "        yaxis=dict( title = 'Formation energy'),\n",
    "        xaxis=dict( title = 'Spacegroup'))\n",
    "    \n",
    "iplot(go.Figure(data=data,layout=layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for number_of_total_atoms in df_train['spacegroup'].value_counts().index.tolist():\n",
    "    y0 = df_train[df_train['spacegroup']==number_of_total_atoms]['bandgap'].values\n",
    "    data.append(go.Box(y=y0, name=str(number_of_total_atoms), boxpoints = 'suspectedoutliers',boxmean='sd'))\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        title = \"Spacegroup vs. Bandgap\",\n",
    "        yaxis=dict( title = 'Bandgap'),\n",
    "        xaxis=dict( title = 'Spacegroup'))\n",
    "    \n",
    "iplot(go.Figure(data=data,layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacegroup 12 shows many statistical outliers for bandgaps. This is a point to keep in mind during regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.heatmap(df_train.corr() ,annot=True,fmt=\".1f\",ax=ax,annot_kws={\"size\": 12}) \n",
    "ax.set_title(\"Correlation heatmap between all columns of training data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap gives an insight into the correlations among all columns. Bandgap shows strong correlation with proportion of Indium (-0.8) and Aluminium (0.7). del_Hf does not have equally strong corelations with any of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Classification of Spacegroups\n",
    "\n",
    "In order to verify that the given structural features in the dataset can correctly define the structure, we are using unit cell lattice parameters as the features to classify the spacegroup of the structures. Because spacegroup is a unique property of the crystal that is calculated using unit cell parameters, a correct prediction of this property will give us confidence in the given structural features of the datasets.<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Scaling using min max via inbuilt function\n",
    "x = df_train.loc[:,['a', 'b', 'c', 'alpha', 'beta', 'gamma', 'N_total']].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(x)\n",
    "y_train = df_train.loc[:,['spacegroup']].values.ravel()\n",
    "df_lattice = pd.DataFrame(X_train)\n",
    "df_lattice.columns=['a', 'b', 'c', 'alpha', 'beta', 'gamma', 'N_total']\n",
    "\n",
    "\n",
    "## Useful functions for assessing the efficiency of the algorithms\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "def plot_confusion_matrix(matrix, ax=None, title=None):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    sns.heatmap(matrix.T, square=True, annot=True, fmt='0.1f', cbar=False, \\\n",
    "                xticklabels=spacegroups, yticklabels=spacegroups, robust=True, ax=ax)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('true spacegroup')\n",
    "    plt.ylabel('predicted spacegroup');\n",
    "    \n",
    "# Function to assess accuracy of a classification algorithm\n",
    "def test_classifier_total(labels_true, labels_predicted):\n",
    "    cm = confusion_matrix(labels_true, labels_predicted)\n",
    "    length_cm = int(cm.shape[0])\n",
    "    total_correct = 0\n",
    "    total_incorrect = 0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if i==j:\n",
    "                total_correct += cm[i][j]\n",
    "            else:\n",
    "                total_incorrect += cm[i][j]\n",
    "    percent_accuracy = total_correct/(total_correct+total_incorrect)*100\n",
    "    return percent_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scree plots for LDA and PCA algorithms for dimensionality reduction\n",
    "At first, LDA and PCA algorithms are implemented to see the number of components required to capture the 95% variance of the data. Following the dimensionality reduction comparison, several classification algorithms are studied and compared to classify the spacegroups for training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "conductors_lda = LinearDiscriminantAnalysis(n_components=7)\n",
    "LDA_conductors = conductors_lda.fit_transform(X_train, df_train.loc[:,['spacegroup']].values.ravel())\n",
    "\n",
    "conductors_pca = PCA(n_components=7)\n",
    "X_PCA = conductors_pca.fit_transform(X_train) \n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,4))\n",
    "ev_pca = conductors_pca.explained_variance_ratio_\n",
    "ev_lda = conductors_lda.explained_variance_ratio_\n",
    "\n",
    "axes[0].plot(range(1, len(ev_lda)+1), np.cumsum(ev_lda))\n",
    "axes[0].plot([0,7],[0.95,0.95])\n",
    "axes[0].set_ylim(0.4,1)\n",
    "axes[0].set_xlabel('number of principle components')\n",
    "axes[0].set_ylabel('variance')\n",
    "axes[0].set_title('LDA')\n",
    "axes[1].plot(range(1, len(ev_pca)+1), np.cumsum(ev_pca))\n",
    "axes[1].plot([0,7],[0.95,0.95])\n",
    "axes[1].set_ylim(0.4,1)\n",
    "axes[1].set_xlabel('number of principle components')\n",
    "axes[1].set_ylabel('variance')\n",
    "axes[1].set_title('PCA')\n",
    "plt.suptitle('Scree plots for 95% variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scree plots show that Linear Discriminant Analysis (LDA) is able to account for more than 95% variance using only 2 components while Principle Component Analysis (PCA) needs 4 components to capture the same variance. It is an expected behavior because LDA discriminates between interclass and intraclass variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA vs PCA (n_components = 2) for spacegroup clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# classification of structures using LDA\n",
    "lda_2 = LinearDiscriminantAnalysis(n_components=2)\n",
    "LDA_conductors = lda_2.fit_transform(X_train, y_train)\n",
    "lda_df = pd.DataFrame(LDA_conductors)\n",
    "lda_df['labels'] = y_train\n",
    "lda_df['labels_predicted'] = conductors_lda.predict(X_train)\n",
    "\n",
    "# classification of spacegroups using PCA\n",
    "conductors_pca = PCA(n_components=2)\n",
    "X_PCA = conductors_pca.fit_transform(X_train) \n",
    "pca_df = pd.DataFrame(X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting LDA and PCA predictions classifying the spacegroups of the training set\n",
    "fig,axes = plt.subplots(1,2, figsize=(12,4))\n",
    "colors = [plt.cm.viridis((i/(n_spacegroups-1))) for i in range(n_spacegroups)]\n",
    "colors[1] = 'r'\n",
    "\n",
    "# Plotting LDA predictions\n",
    "xpos=0\n",
    "for label in range(n_spacegroups):\n",
    "    idxs = np.array(lda_df['labels'] == spacegroups[label])\n",
    "    c = colors[label]\n",
    "    axes[0].scatter(lda_df.iloc[idxs,0], lda_df.iloc[idxs,1], color=c, alpha=0.8)\n",
    "    axes[0].annotate(str(spacegroups[label]), xy=[xpos, 0.95], xycoords='axes fraction', color=c, size=15)\n",
    "    axes[0].set_title('LDA')\n",
    "    xpos += 0.1\n",
    "    \n",
    "# Plotting PCA predictions\n",
    "xpos=0\n",
    "for label in range(n_spacegroups):\n",
    "    idxs = np.array(lda_df['labels'] == spacegroups[label])\n",
    "    c = colors[label]\n",
    "    axes[1].scatter(pca_df.iloc[idxs,0], pca_df.iloc[idxs,1], color=c, alpha=0.8)\n",
    "    axes[1].annotate(str(spacegroups[label]), xy=[xpos, 0.95], xycoords='axes fraction', color=c, size=15)\n",
    "    axes[1].set_title('PCA')\n",
    "    xpos += 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separate clusters for spacegroups are easily observed using only 2 components LDA algorithm while PCA algorithm is not able to capture the required variance along 2 principle components. It shows that LDA performs much better than PCA for dimensinality reduction and clustering in our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification of Spacegroups using different algorithms<br>\n",
    "We used LDA, KNeighbors Classifier (KNC) and Support Vector Machines (SVM) are implemented for spacegroup classification in the training dataset and the fitted models are used to predict the spacegroups in the test dataset. Performance of all the algorithm is compared based the their accuracy defined as: \n",
    "\n",
    "$$Accuracy=\\frac{total\\ correct\\ predictions}{total\\ correct\\ predictions + total\\ incorrect\\ predictions}*100$$<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,5,figsize=(16,4))\n",
    "\n",
    "cm_lda = confusion_matrix(lda_df['labels'], lda_df['labels_predicted'])   \n",
    "plot_confusion_matrix(cm_lda, ax=axes[0])\n",
    "axes[0].set_title('LDA')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "\n",
    "# Applying KNC algorithm to fit training dataset for spacegroups\n",
    "knc_3 = KNC(n_neighbors=3)\n",
    "knc_3.fit(X_train, y_train)\n",
    "y_knc = knc_3.predict(X_train)\n",
    "cm_knc = confusion_matrix(y_train, y_knc)\n",
    "plot_confusion_matrix(cm_knc, ax=axes[1])\n",
    "axes[1].set_title('KNC')\n",
    "\n",
    "\n",
    "# Applying Linear SVM algorithm to fit training dataset for spacegroups\n",
    "linearSVM = SVC(kernel='linear')\n",
    "linearSVM.fit(X_train, y_train)\n",
    "y_lsvm = linearSVM.predict(X_train)\n",
    "cm_lsvm = confusion_matrix(y_train, y_lsvm)\n",
    "plot_confusion_matrix(cm_lsvm, ax=axes[2])\n",
    "axes[2].set_title('LSVM')\n",
    "\n",
    "\n",
    "# Applying Polynomial SVM algorithm to fit training dataset for spacegroups\n",
    "polySVM = SVC(kernel='poly', degree=2)\n",
    "polySVM.fit(X_train, y_train)\n",
    "y_psvm = polySVM.predict(X_train)\n",
    "cm_psvm = confusion_matrix(y_train, y_psvm)\n",
    "plot_confusion_matrix(cm_psvm, ax=axes[3])\n",
    "axes[3].set_title('PSVM')\n",
    "\n",
    "\n",
    "# Applying Radial Basis Function SVM algorithm to fit training dataset for spacegroups\n",
    "rbfSVM = SVC(kernel = 'rbf')\n",
    "rbfSVM.fit(X_train, y_train)\n",
    "y_rsvm = rbfSVM.predict(X_train)\n",
    "cm_rsvm = confusion_matrix(y_train, y_rsvm)\n",
    "plot_confusion_matrix(cm_rsvm, ax=axes[4])\n",
    "axes[4].set_title('rbfSVM')\n",
    "\n",
    "plt.suptitle (\"Confusion matrices of different classification algorithms\")\n",
    "plt.show()\n",
    "\n",
    "print('Accurancy of the LDA model in the training dataset (%): ',test_classifier_total(lda_df['labels'], lda_df['labels_predicted']))\n",
    "print('Accurancy of KNC in the training dataset (%): ',test_classifier_total(y_train, y_knc))\n",
    "print('Accurancy of linear SVM in the training dataset (%): ',test_classifier_total(y_train, y_lsvm))\n",
    "print('Accurancy of polynomial SVM in the training dataset (%): ',test_classifier_total(y_train, y_psvm))\n",
    "print('Accurancy of rbf SVM in the training dataset (%): ',test_classifier_total(y_train, y_rsvm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is hard to visualize multidimensional feature space, we plotted the first two LDA components to show the clusters classified by different algorithms: LDA, KNC, SVMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting spacegroups classification by different algorithms along LDA's 2 principle components \n",
    "fig,axes = plt.subplots(2,3, figsize=(16,8))\n",
    "colors = [plt.cm.viridis((i/(n_spacegroups-1))) for i in range(n_spacegroups)]\n",
    "colors[1] = 'r'\n",
    "\n",
    "\n",
    "# Plotting Actual spacegroups\n",
    "xpos=0\n",
    "for label in range(n_spacegroups):\n",
    "    idxs = np.array(lda_df['labels'] == spacegroups[label])\n",
    "    c = colors[label]\n",
    "    axes[0][0].scatter(lda_df.iloc[idxs,0], lda_df.iloc[idxs,1], color=c, alpha=0.8)\n",
    "    axes[0][0].annotate(str(spacegroups[label]), xy=[xpos, 0.95], xycoords='axes fraction', color=c, size=15)\n",
    "    axes[0][0].set_title('Actual')\n",
    "    xpos += 0.1\n",
    "    \n",
    "\n",
    "# Plotting LDA predictions\n",
    "xpos=0\n",
    "for label in range(n_spacegroups):\n",
    "    idxs = np.array(lda_df['labels_predicted'] == spacegroups[label])\n",
    "    c = colors[label]\n",
    "    axes[0][1].scatter(lda_df.iloc[idxs,0], lda_df.iloc[idxs,1], color=c, alpha=0.8)\n",
    "    axes[0][1].annotate(str(spacegroups[label]), xy=[xpos, 0.95], xycoords='axes fraction', color=c, size=15)\n",
    "    axes[0][1].set_title('LDA')\n",
    "    xpos += 0.1\n",
    "    \n",
    "# Plotting KNC predictions\n",
    "xpos=0\n",
    "for label in range(n_spacegroups):\n",
    "    idxs = np.array(y_knc == spacegroups[label])\n",
    "    c = colors[label]\n",
    "    axes[0][2].scatter(lda_df.iloc[idxs,0], lda_df.iloc[idxs,1], color=c, alpha=0.8)\n",
    "    axes[0][2].annotate(str(spacegroups[label]), xy=[xpos, 0.95], xycoords='axes fraction', color=c, size=15)\n",
    "    axes[0][2].set_title('KNC')\n",
    "    xpos += 0.1\n",
    "    \n",
    "# Plotting LSVM predictions\n",
    "xpos=0\n",
    "for label in range(n_spacegroups):\n",
    "    idxs = np.array(y_lsvm == spacegroups[label])\n",
    "    c = colors[label]\n",
    "    axes[1][0].scatter(lda_df.iloc[idxs,0], lda_df.iloc[idxs,1], color=c, alpha=0.8)\n",
    "    axes[1][0].annotate(str(spacegroups[label]), xy=[xpos, 0.95], xycoords='axes fraction', color=c, size=15)\n",
    "    axes[1][0].set_title('LSVM')\n",
    "    xpos += 0.1\n",
    "    \n",
    "# Plotting PSVM predictions\n",
    "xpos=0\n",
    "for label in range(n_spacegroups):\n",
    "    idxs = np.array(y_psvm == spacegroups[label])\n",
    "    c = colors[label]\n",
    "    axes[1][1].scatter(lda_df.iloc[idxs,0], lda_df.iloc[idxs,1], color=c, alpha=0.8)\n",
    "    axes[1][1].annotate(str(spacegroups[label]), xy=[xpos, 0.95], xycoords='axes fraction', color=c, size=15)\n",
    "    axes[1][1].set_title('PSVM')\n",
    "    xpos += 0.1\n",
    "    \n",
    "# Plotting rbf SVM predictions\n",
    "xpos=0\n",
    "for label in range(n_spacegroups):\n",
    "    idxs = np.array(y_rsvm == spacegroups[label])\n",
    "    c = colors[label]\n",
    "    axes[1][2].scatter(lda_df.iloc[idxs,0], lda_df.iloc[idxs,1], color=c, alpha=0.8)\n",
    "    axes[1][2].annotate(str(spacegroups[label]), xy=[xpos, 0.95], xycoords='axes fraction', color=c, size=15)\n",
    "    axes[1][2].set_title('rbf SVM')\n",
    "    xpos += 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction of test dataset spacegroups using pre-fitted models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected top 3 performing classification algorithms to predict spacegroups of the test dataset. Such comparison would allow us to check the robustness against high variance (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(16,4))\n",
    "\n",
    "# Importing and scaling the dataset\n",
    "x_test = df_test.loc[:,['a', 'b', 'c', 'alpha', 'beta', 'gamma', 'N_total']].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled_test = min_max_scaler.fit_transform(x_test)\n",
    "y_actual = df_test.loc[:,['spacegroup']].values\n",
    "\n",
    "# Applying the trained LDA model for test dataset prediction\n",
    "y_lda = lda_2.predict(x_scaled_test)\n",
    "cm_lda = confusion_matrix(y_actual, y_lda)\n",
    "plot_confusion_matrix(cm_lda, ax=axes[0])\n",
    "axes[0].set_title('LDA')\n",
    "print('Accurancy of LDA model in test dataset predication (%): ',test_classifier_total(y_actual, y_lda))\n",
    "\n",
    "# Applying the trained KNC model for test dataset prediction\n",
    "y_knn = knc_3.predict(x_scaled_test)\n",
    "cm_knn = confusion_matrix(y_actual, y_knn)\n",
    "plot_confusion_matrix(cm_knn, ax=axes[1])\n",
    "axes[1].set_title('KNC')\n",
    "print('Accurancy of KNC model in test dataset prediction (%): ',test_classifier_total(y_actual, y_knn))\n",
    "\n",
    "# Applying the trained LSVM model for test dataset prediction\n",
    "y_lsvm = linearSVM.predict(x_scaled_test)\n",
    "cm_lsvm = confusion_matrix(y_actual, y_lsvm)\n",
    "plot_confusion_matrix(cm_lsvm, ax=axes[2])\n",
    "axes[2].set_title('LSVM')\n",
    "print('Accurancy of linear SVM in the training dataset (%): ',test_classifier_total(y_actual, y_lsvm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> The analysis above shows that <u> KNC is the best performing algorithm </u> for spacegroup classification which predicts the spacegroups of the test dataset with 100% accuracy. </i>\n",
    "Additionally, the performance of all three algorithms on test dataset shows that the learners were not over-fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Regression to predict Formation Energies and Bandgaps\n",
    "\n",
    "We will explore different regression strategies to predict del_Hf and bandgaps. \n",
    "<br> Let us start by splitting our data into training and validation dataset and removing spacegroups from the dataset. Additionally, we will remove del_Hf and bandgaps from valdiation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "t1 = 'del_Hf'\n",
    "t2 = 'bandgap'\n",
    "\n",
    "transform_columns = ['N_total','x_Al','x_Ga','x_In','a','b','c','alpha', 'beta','gamma']\n",
    "feature_columns = ['spacegroup'] + transform_columns\n",
    "\n",
    "all = pd.concat([df_train[feature_columns], df_test])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "scaler.fit(all[transform_columns])\n",
    "\n",
    "df_train[transform_columns] = scaler.transform(df_train[transform_columns])\n",
    "df_test[transform_columns] = scaler.transform(df_test[transform_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are choosing a 80-20 split between training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=5776\n",
    "X_train, X_validation = train_test_split(df_train, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "y_train = np.log1p(X_train[[t1, t2]])\n",
    "X_train = X_train.drop([t1, t2], axis=1)\n",
    "\n",
    "y_validation = np.log1p(X_validation[[t1, t2]])\n",
    "X_validation = X_validation.drop([ t1, t2], axis=1)\n",
    "\n",
    "print(\"The shapes of X_train and y_train are:\")\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"The shapes of X_validation and y_validation are:\")\n",
    "print(X_validation.shape, y_validation.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up a pipeline to test SciKit Learners\n",
    "\n",
    "Setting up a pipeline will allow us to easily implement differnt algorithms for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using KNN regressor right now, but setting up a Pipeline to test for future\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('knn', KNeighborsRegressor()),\n",
    "    ])  #We can add more learners here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KNN regressor, we will train a learner to predict del_Hf and bandgaps. To avoid overfitting, we will use GridSearch CV with 20-fold validation strategy. The large size of dataset precludes a higher fold validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cv_folds=20\n",
    "#Hyperparameter tuning using inbuilt GridSeachCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters = { \n",
    "                    'knn__n_neighbors': [2, 4, 7, 10, 12, 15]\n",
    "                  }\n",
    "model_knn = GridSearchCV(pipeline, hyperparameters, cv = num_cv_folds)\n",
    " \n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "pred = model_knn.predict(X_train)  #Using train_data right now to see how good is the performance\n",
    "pred_vals = pd.DataFrame(pred)\n",
    "pred_vals.columns = [t1,t2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantifying Complexity\n",
    "\n",
    "We are using default inverse GridCV scores for quantifying complexity. The lower the inverse score, better is the model, i.e. no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_cv_scores(model,hyperparameters,param_name='KNN_Neighbours'):\n",
    "    scores_sd=-1*model.cv_results_[\"mean_test_score\"]\n",
    "    \n",
    "    fig,ax=plt.subplots(1,1)\n",
    "    ax.plot(hyperparameters,scores_sd)\n",
    "    ax.set_ylabel('-Grid CV Score')\n",
    "    ax.set_xlabel(param_name)\n",
    "    plt.suptitle('Grid CV Score vs '+param_name)\n",
    "    plt.show()\n",
    "plot_grid_cv_scores(clf,hyperparameters['knn__n_neighbors'])  \n",
    "print(\"The optimum number of parameters is: {}\".format(clf.best_params_))\n",
    "print()   #Optimum neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parity plots can be a good visualization to analyse the performance of regressors. Below is the parity plot for training data using KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Parity Plots \n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "plt.suptitle('Performance of KNN with best hyperparameters on training data', fontsize=16)\n",
    "yp=y_train['del_Hf']\n",
    "ax[0].scatter(y_train['del_Hf'], pred_vals['del_Hf'], color='k', alpha=0.2)\n",
    "ax[0].plot([min(yp), max(yp)], [min(yp), max(yp)], ls='-', color='r') #<- this is called a parity plot\n",
    "ax[0].set_xlabel('Actual Formation Energy (Standardized)')\n",
    "ax[0].set_ylabel('Predicted Formation Energy (Standardized)')\n",
    "\n",
    "yp=y_train['bandgap']\n",
    "ax[1].scatter(y_train['bandgap'], pred_vals['bandgap'], color='r', alpha=0.2)\n",
    "ax[1].plot([min(yp), max(yp)], [min(yp), max(yp)], ls='-', color='k') #<- this is called a parity plot\n",
    "ax[1].set_xlabel('Actual bandgap (Standardized)')\n",
    "ax[1].set_ylabel('Predicted bandgap (Standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar plot is seen for validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation = clf.predict(X_validation)  #Using train_data right now to see how good is the performance\n",
    "pred_vals_validation = pd.DataFrame(pred_validation)\n",
    "pred_vals_validation.columns = [t1,t2]\n",
    "\n",
    "#Parity Plots \n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "plt.suptitle('Performance of KNN with best hyperparameters on validation set', fontsize=16)\n",
    "yp=y_validation['del_Hf']\n",
    "ax[0].scatter(y_validation['del_Hf'], pred_vals_validation['del_Hf'], color='k', alpha=0.2)\n",
    "ax[0].plot([min(yp), max(yp)], [min(yp), max(yp)], ls='-', color='r') #<- this is called a parity plot\n",
    "ax[0].set_xlabel('Actual Formation Energy (Standardized)')\n",
    "ax[0].set_ylabel('Predicted Formation Energy (Standardized)')\n",
    "\n",
    "yp=y_validation['bandgap']\n",
    "ax[1].scatter(y_validation['bandgap'], pred_vals_validation['bandgap'], color='r', alpha=0.2)\n",
    "ax[1].plot([min(yp), max(yp)], [min(yp), max(yp)], ls='-', color='k') #<- this is called a parity plot\n",
    "ax[1].set_xlabel('Actual bandgap (Standardized)')\n",
    "ax[1].set_ylabel('Predicted bandgap (Standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a regression problem, error metrics need to be defined to assess the algorithm performance. Here, we will use R<sup>2</sup> and standard error. Here we will write functions to calculate these quantitites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantifying error in regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error  #, mean_squared_log_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantifying Error - mean of absolute deviation of errors\n",
    "def std_err(model, X, y, type_=0):\n",
    "    yhat = model.predict(X)\n",
    "    return np.mean(np.abs(y[:,type_] - yhat[:,type_]))\n",
    "\n",
    "#def CVscore(model, X, y, cv):\n",
    "#    scores = cross_val_score(model, X, y, cv=cv)\n",
    "    #print(scores.min(), scores.max())\n",
    "#    return (np.mean(scores))\n",
    "\n",
    "def r2score(model, X, y_true, type_=0):\n",
    "    y_pred=model.predict(X)\n",
    "    return r2_score(y_true[:,type_], y_pred[:,type_]) \n",
    "    #print(scores.min(), scores.max())\n",
    "    return (np.mean(scores))\n",
    "\n",
    "def performance_metrics(model,X,y,cv=3, type_=0): #type_0 is for del_Hf and type_1 is for bandgap \n",
    "    y1 = y#[:,type_]\n",
    "    model_new=model#.fit(X,y1)\n",
    "    std_error=std_err(model_new,X,y1, type_=type_)\n",
    "    R2_score =r2score(model_new,X,y1, type_=type_)\n",
    "    #CV_score =CVscore(model_new,X,y1,cv=cv)\n",
    "    return R2_score,std_error#,CV_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defined functions are used to see the performance of KNN regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(std_err(clf,X_train,y_train))\n",
    "print('Performance metrics of del_Hf predictions using KNN\\n')\n",
    "print(\"Training data:   \",performance_metrics(model_knn,X_train.values,y_train.values, type_=0))\n",
    "print(\"Validation data: \",performance_metrics(model_knn,X_validation.values,y_validation.values, type_=0))\n",
    "print('\\nPerformance metrics of bandgap predictions using KNN\\n')\n",
    "print(\"Training data:   \",performance_metrics(model_knn,X_train.values,y_train.values, type_=1))\n",
    "print(\"Validation data: \",performance_metrics(model_knn,X_validation.values,y_validation.values, type_=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> KNN seems to work better for bandgap predictions. The equivalent performance on validation data is a proof that the regressor was without high variance. This was further expected since it was trained using optimum complexity via CV score. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Analysing other regressors\n",
    "\n",
    "As a part of this project, we are also using other learners viz. linear, polynomial and random forest regressors (RFR). Their model objects are declared below. Additionally, RFR regressor is optimized using GridSearch CV to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Linear\n",
    "model_linear     = Pipeline([('linear', LinearRegression(fit_intercept=True))])\n",
    "model_linear.fit(X_train,y_train)\n",
    "#2nd order poly\n",
    "model_polyr_two  = Pipeline([('poly', PolynomialFeatures(degree=2)),('linear', LinearRegression(fit_intercept=False))])\n",
    "model_polyr_two.fit(X_train,y_train)\n",
    "#3rd order poly\n",
    "model_polyr_three= Pipeline([('poly', PolynomialFeatures(degree=3)),('linear', LinearRegression(fit_intercept=False))])\n",
    "model_polyr_three.fit(X_train,y_train)\n",
    "#RFR\n",
    "pipeline_rfr         = Pipeline([('rfr', RandomForestRegressor()),])  #We can add more learners here.\n",
    "hyperparameters_rfr  = {'rfr__min_samples_leaf':[2,4,6],'rfr__min_samples_split': [4,8,12],'rfr__n_estimators': [400]}\n",
    "model_rfr        = GridSearchCV(pipeline_rfr, hyperparameters_rfr)\n",
    "model_rfr.fit(X_train, y_train)\n",
    "print(\"The optimum number of parameters is: {}\".format(model_rfr.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics of each algorithm on training and valdiation data is evaluated below. Firstly, performance in predicting del_Hf is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[model_knn,model_linear,model_polyr_two,model_polyr_three,model_rfr]\n",
    "model_list_str = ['KNN', 'Linear', 'Poly_two', 'Poly_three', 'RFR']\n",
    "columns_str = ['R2_train','Std_Err_train',  'R2_validation','Std_Err_validation']\n",
    "columns_str2 = ['R2_train', 'R2_validation','Std_Err_train', 'Std_Err_validation']\n",
    "\n",
    "#model_list=[model_rfr]\n",
    "del_Hf_performance = []\n",
    "bandgap_performance = []\n",
    "for model in model_list:\n",
    "    del_Hf_local = [x for x in performance_metrics(model,X_train.values,y_train.values, type_=0)] \\\n",
    "    + [x for x in performance_metrics(model,X_validation.values,y_validation.values, type_=0)]\n",
    "    del_Hf_performance.append(del_Hf_local)    \n",
    "    \n",
    "    bandgap_local = [x for x in performance_metrics(model,X_train.values,y_train.values, type_=1)] \\\n",
    "    + [x for x in performance_metrics(model,X_validation.values,y_validation.values, type_=1)]\n",
    "    bandgap_performance.append(bandgap_local)\n",
    "\n",
    "performance_df=pd.DataFrame(index=model_list_str, data=del_Hf_performance,columns=columns_str)\n",
    "performance_df.reindex(columns=columns_str2)\n",
    "performance_df.sort_values(by=['R2_validation'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the learners is able to predict del_Hf with very high accuracy. Within the learners, Poly_three and poly_two show signs of overfitting due to their difference in R<sup>2</sup> scores between training and validation data. Additionally, for future work better features may be explored for del_Hf predictions.\n",
    "\n",
    "The performance metrics for bandgap predictions are shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_bg_df=pd.DataFrame(index=model_list_str, data=bandgap_performance,columns=columns_str)\n",
    "performance_bg_df.reindex(columns=columns_str2)\n",
    "performance_bg_df.sort_values(by=['R2_validation'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poly_two, KNN and RFR seem to present equivalent performances. However, with multiple runs with different random states, RFR tends to perform better. Additionally, RFR performance on training data is the best amongst all considered algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_best=model_rfr\n",
    "pred = model_best.predict(X_train)  #Using train_data right now to see how good is the performance\n",
    "pred_vals = pd.DataFrame(pred)\n",
    "pred_vals.columns = [t1,t2]\n",
    "\n",
    "\n",
    "\n",
    "#Parity Plots \n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "plt.suptitle('Performance of RFR on training and valdiation data', fontsize=16)\n",
    "yp=y_train['bandgap']\n",
    "ax[0].scatter(y_train['bandgap'], pred_vals['bandgap'], color='r', alpha=0.2)\n",
    "ax[0].plot([min(yp), max(yp)], [min(yp), max(yp)], ls='-', color='k') #<- this is called a parity plot\n",
    "ax[0].set_xlabel('Actual bandgap (Standardized)')\n",
    "ax[0].set_ylabel('Predicted bandgap (Standardized)')\n",
    "\n",
    "yp=y_validation['bandgap']\n",
    "ax[1].scatter(y_validation['bandgap'], pred_vals_validation['bandgap'], color='r', alpha=0.2)\n",
    "ax[1].plot([min(yp), max(yp)], [min(yp), max(yp)], ls='-', color='k') #<- this is called a parity plot\n",
    "ax[1].set_xlabel('Actual bandgap (Standardized)')\n",
    "ax[1].set_ylabel('Predicted bandgap (Standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parity plots for bandgap prediction using RFR learner show good results.\n",
    "\n",
    "Better features and learners, or the use of ensemble learning could help improve the results for del_Hf prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "plt.suptitle('Performance of RFR on training and valdiation data', fontsize=16)\n",
    "yp=y_train['del_Hf']\n",
    "ax[0].scatter(y_train['del_Hf'], pred_vals['del_Hf'], color='k', alpha=0.2)\n",
    "ax[0].plot([min(yp), max(yp)], [min(yp), max(yp)], ls='-', color='r') #<- this is called a parity plot\n",
    "ax[0].set_xlabel('Actual bandgap (Standardized)')\n",
    "ax[0].set_ylabel('Predicted bandgap (Standardized)')\n",
    "\n",
    "yp=y_validation['del_Hf']\n",
    "ax[1].scatter(y_validation['del_Hf'], pred_vals_validation['del_Hf'], color='k', alpha=0.2)\n",
    "ax[1].plot([min(yp), max(yp)], [min(yp), max(yp)], ls='-', color='r') #<- this is called a parity plot\n",
    "ax[1].set_xlabel('Actual bandgap (Standardized)')\n",
    "ax[1].set_ylabel('Predicted bandgap (Standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. T. Mueller, A.G. Kusne, and R. Ramprasad, <i>Rev. in Comp. Chem.</i>, 29 (2016).\n",
    "\n",
    "2. R. LeSar, <i>Stat. Anal. Data Min.</i>, 1, 372 (2009). \n",
    "\n",
    "3. C. J. Long, J. Hattrick‐Simpers, M. Murakami, R. C. Srivastava, I. Takeuchi, V. L. Karen, and X. Li, <i>Rev. Sci. Instrum.</i>, 78, 072217 (2007). \n",
    "\n",
    "4. G. Hautier, C. C. Fischer, A. Jain, T. Mueller, and G. Ceder,<i> Chem. Mater.</i>, 22, 3762 (2010).\n",
    "\n",
    "5. D. Morgan, S. Curtarolo, K. Persson, J. Rodgers, and G. Ceder, <i>Phys. Rev. Lett.</i>, 91, 135503 (2003). \n",
    "\n",
    "6. G. Pilania, C. Wang, X. Jiang, S. Rajasekaran, and R. Ramprasad, <i>Sci. Rep.</i>, 3, 1 (2013).\n",
    "\n",
    "7. K. Hansen, G. Montavon, F. Biegler, S. Fazli, M. Rupp, M. Scheffler, O. A. von Lilienfeld, A. Tkatchenko, and K.‐R. Müller,<i> J. Chem. Theory Comput.</i>, 9, 3404 (2013). \n",
    "\n",
    "8. V. Botu and R. Ramprasad, <i>Int. J. Quantum Chem.</i>, 115, 1074 (2015). \n",
    "\n",
    "9. From Wikipedia, url: https://en.wikipedia.org/wiki/Transparent_conducting_film (Last accessed: February 15, 2018)\n",
    "\n",
    "10. <i>Introduction to Solid State Physics 8th Edition</i>,  Kittel C. (2005), John Wiley and Sons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
